# Why can't we use a linear activation function for deep neural networks?

If we only use linear activation functions, g(z) = z, regardless the number of layers we have in a neural network, the final output will still behave like a single linear function such as stacking multiple linear functions is like doing one linear function for all. We can still use a linear activation function for other use cases such as below:

### Linear activation
Use case: we can use it to predict continuous variables like house prices, stock prices, or temperature. 
Why: predicting house prices given features like area code, number of rooms, and location, etc. 

